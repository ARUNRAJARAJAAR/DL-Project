{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5HdhdjfVDc5o",
        "HZVo4IxMeAGI",
        "I_-5vfgQX5SW",
        "chI5LrSNdfpC",
        "UGR_YD6OYPs6",
        "3xX5nZOWh8uU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0da9cfb5749f4e7ea5929607e88f531f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2108d706787340ab8aed388551a5f74a",
              "IPY_MODEL_36ef3a731a024bfaa78d55a662f66d18",
              "IPY_MODEL_41080295b27d4198988ec75f0a7d2a96"
            ],
            "layout": "IPY_MODEL_6e9e0cde29dc41f7b4b4a8bc458462fa"
          }
        },
        "2108d706787340ab8aed388551a5f74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ab799ab2d474ea19bbe1e69287483a0",
            "placeholder": "​",
            "style": "IPY_MODEL_83d9647b3aaf4cbd84da51b6ca14ea40",
            "value": "config.json: 100%"
          }
        },
        "36ef3a731a024bfaa78d55a662f66d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eb1f75847324e1da58b384bb9d15f1d",
            "max": 2222,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e21d17069c64761b422c4306121f6c9",
            "value": 2222
          }
        },
        "41080295b27d4198988ec75f0a7d2a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80d505bd20a547eda1d0e5c1d523c9b0",
            "placeholder": "​",
            "style": "IPY_MODEL_d1e48faffbbf472497a304015816e4d1",
            "value": " 2.22k/2.22k [00:00&lt;00:00, 55.1kB/s]"
          }
        },
        "6e9e0cde29dc41f7b4b4a8bc458462fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ab799ab2d474ea19bbe1e69287483a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83d9647b3aaf4cbd84da51b6ca14ea40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eb1f75847324e1da58b384bb9d15f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e21d17069c64761b422c4306121f6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80d505bd20a547eda1d0e5c1d523c9b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e48faffbbf472497a304015816e4d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c2ab7f4517e4a938a644572205f4a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82b5d76b096e4534a7cfeec52e2e3991",
              "IPY_MODEL_5ad8d4b101664f9ba1bebb44c77c36b4",
              "IPY_MODEL_52913fa174a345dd9db79bbd91982889"
            ],
            "layout": "IPY_MODEL_787423274d984c5b8cc9e1cb33ddf16a"
          }
        },
        "82b5d76b096e4534a7cfeec52e2e3991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcdacc192df44791b56d6fdfbde1e309",
            "placeholder": "​",
            "style": "IPY_MODEL_6cb9b5b119ee4e55b98e766eb9198298",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "5ad8d4b101664f9ba1bebb44c77c36b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b39f65cb09c49a0ad46492f370a96ca",
            "max": 1261990257,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0328842655fc4c3bbf1210430e356b6d",
            "value": 1261990257
          }
        },
        "52913fa174a345dd9db79bbd91982889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c993d9990f448afbd9ed178f8a2ecf1",
            "placeholder": "​",
            "style": "IPY_MODEL_360d03e929d04fddb4432def94ee8e22",
            "value": " 1.26G/1.26G [00:08&lt;00:00, 28.5MB/s]"
          }
        },
        "787423274d984c5b8cc9e1cb33ddf16a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcdacc192df44791b56d6fdfbde1e309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb9b5b119ee4e55b98e766eb9198298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b39f65cb09c49a0ad46492f370a96ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0328842655fc4c3bbf1210430e356b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c993d9990f448afbd9ed178f8a2ecf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "360d03e929d04fddb4432def94ee8e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afff6630b3d5447093e4c73fb62d9e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10ddc70b280c4ba083f148c1d0c722da",
              "IPY_MODEL_7c8fd9555295460fbcbd21ef7089a3d8",
              "IPY_MODEL_0c661e736d7c4cb78bbac2c0916ef5ea"
            ],
            "layout": "IPY_MODEL_d9f7c8b5a72c47529ca7f5fc7873a68d"
          }
        },
        "10ddc70b280c4ba083f148c1d0c722da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6825e80446a1425898647160ee7fe274",
            "placeholder": "​",
            "style": "IPY_MODEL_1175930ebb7a40ef80be881593f0a5ee",
            "value": "model.safetensors: 100%"
          }
        },
        "7c8fd9555295460fbcbd21ef7089a3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8930060eca964740ad369b87c83d18d0",
            "max": 1261869072,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6994a48630fd4ab89b387376986ce5a5",
            "value": 1261869072
          }
        },
        "0c661e736d7c4cb78bbac2c0916ef5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_781234e9db61401096ef22919c683c9c",
            "placeholder": "​",
            "style": "IPY_MODEL_c78042474eda40f48d6f04c4c25ca687",
            "value": " 1.26G/1.26G [00:14&lt;00:00, 26.3MB/s]"
          }
        },
        "d9f7c8b5a72c47529ca7f5fc7873a68d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6825e80446a1425898647160ee7fe274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1175930ebb7a40ef80be881593f0a5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8930060eca964740ad369b87c83d18d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6994a48630fd4ab89b387376986ce5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "781234e9db61401096ef22919c683c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c78042474eda40f48d6f04c4c25ca687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This cell handles the initial setup and imports all necessary dependencies for the SSL-TTS framework:\n",
        "- Clones the TTS repository from coqui-ai\n",
        "- Installs required packages: TTS, transformers, torchaudio\n",
        "- Imports core deep learning libraries (torch, torchaudio)\n",
        "- Imports WavLM model for SSL feature extraction\n",
        "- Imports GlowTTS components for the text-to-SSL model\n",
        "- Sets up other essential utilities like torch.nn.functional"
      ],
      "metadata": {
        "id": "WvGacQEGY8dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/coqui-ai/TTS.git\n",
        "%cd TTS\n",
        "\n",
        "\n",
        "!pip install TTS transformers torchaudio\n",
        "!pip install mutagen\n",
        "# DO NOT RESTART RUNTIME AFTER RUNNING THIS CELL\n",
        "# YOU MIGHT HAVE A FEW WARNINGS/ERROR BUT DW IT'S FINE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xe8HZEkTVN0Y",
        "outputId": "0c00204b-a518-465e-a7ff-481830a0509e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TTS'...\n",
            "remote: Enumerating objects: 32844, done.\u001b[K\n",
            "remote: Counting objects: 100% (3521/3521), done.\u001b[K\n",
            "remote: Compressing objects: 100% (126/126), done.\u001b[K\n",
            "remote: Total 32844 (delta 3433), reused 3395 (delta 3395), pack-reused 29323 (from 1)\u001b[K\n",
            "Receiving objects: 100% (32844/32844), 166.12 MiB | 12.86 MiB/s, done.\n",
            "Resolving deltas: 100% (23858/23858), done.\n",
            "/content/TTS\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.11)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.13.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.5.0+cu121)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.2.post1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.2)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (7.4.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (4.66.6)\n",
            "Collecting anyascii>=0.3.0 (from TTS)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.10.10)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.0.3)\n",
            "Collecting pysbd>=0.3.4 (from TTS)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting umap-learn>=0.5.1 (from TTS)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas<2.0,>=1.4 (from TTS)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.0)\n",
            "Collecting trainer>=0.0.32 (from TTS)\n",
            "  Downloading trainer-0.0.36-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting coqpit>=0.0.16 (from TTS)\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Collecting pypinyin (from TTS)\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hangul-romanize (from TTS)\n",
            "  Downloading hangul_romanize-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gruut==2.2.3 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jamo (from TTS)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.9.1)\n",
            "Collecting g2pkk>=0.1.1 (from TTS)\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bangla (from TTS)\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting bnnumerizer (from TTS)\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bnunicodenormalizer (from TTS)\n",
            "  Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.8.0)\n",
            "Collecting encodec>=0.1.1 (from TTS)\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode>=1.3.2 (from TTS)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting num2words (from TTS)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.10/dist-packages (from spacy[ja]>=3->TTS) (3.7.5)\n",
            "Collecting numpy==1.22.0 (from TTS)\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.60.0)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.16.0)\n",
            "Collecting dateparser~=1.1.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting gruut-ipa<1.0,>=0.12.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_en-2.0.1.tar.gz (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-crfsuite~=0.9.7 (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting gruut_lang_es~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_es-2.0.1.tar.gz (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_fr~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_fr-2.0.2.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_de~=2.0.0 (from gruut[de,es,fr]==2.2.3->TTS)\n",
            "  Downloading gruut_lang_de-2.0.1.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->TTS) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1->TTS) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->TTS) (4.0.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=5.6.0->TTS) (4.4.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa>=0.10.0 (from TTS)\n",
            "  Downloading librosa-0.10.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post2-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0.post1-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading librosa-0.10.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->TTS)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.0->TTS) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0,>=1.4->TTS) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->TTS) (3.5.0)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=1.11.2 (from TTS)\n",
            "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.13.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting sudachipy!=0.6.1,>=0.5.2 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from spacy[ja]>=3->TTS)\n",
            "  Downloading SudachiDict_core-20241021-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from trainer>=0.0.32->TTS) (2.17.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.1->TTS)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "INFO: pip is looking at multiple versions of contourpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->TTS)\n",
            "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1->TTS) (3.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.10.0->TTS) (4.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.23.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.0.5)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp>=3.8.1->TTS) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (4.25.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Downloading TTS-0.22.0-cp310-cp310-manylinux1_x86_64.whl (938 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trainer-0.0.36-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Downloading bnunicodenormalizer-0.1.7-py3-none-any.whl (23 kB)\n",
            "Downloading hangul_romanize-0.1.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiDict_core-20241021-py3-none-any.whl (72.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gruut, encodec, bnnumerizer, docopt, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut_lang_es, gruut_lang_fr\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75788 sha256=61defee74d8e345c8099f816a66fb3eeb87b772fa52cbd72811d97a08ddf8829\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45760 sha256=113987f013ab8b8ce9d70512dd27c0633c50a2dbd1b03b1ac35b74ee31033294\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5261 sha256=a95ab282a32015953f7e9ec1e8f3c274f5d421a43ddcff3475b1ab17d777b10b\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=02dae665dd3d3e37c5afd30219e5cd0a64ae02af68eebeaf9ff886f195cf335b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104873 sha256=bdf612a1fb38432fce46cddffd9f198b2b3f47b14794d04fef4589fd2c796379\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.1-py3-none-any.whl size=18498313 sha256=0c5e38ba73b0cde7e9c4402fbec209f5817cfbcef9560489a3db01cfe68d6c12\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/80/5f/775b357ae61d7cb68793327c7470d848715cbc60bb373af8dd\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.1-py3-none-any.whl size=15326857 sha256=81aeb4441b86dabb8e4d833d441a615c5e344e10fca4992d0ec1ac481d32515c\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8d/b7/d484d224facd899ed188e00374f25dd3f19d1a3f53da6517bd\n",
            "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_es: filename=gruut_lang_es-2.0.1-py3-none-any.whl size=32173928 sha256=e7ee10194871ae47dfc7ba96ecf722071455aa0e2406de3e969616e435092c4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/bd/96/5ddde14e8e6932a96f12c5ab5de62b619d39e2507d7daf5188\n",
            "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_fr: filename=gruut_lang_fr-2.0.2-py3-none-any.whl size=10968766 sha256=312baf39a746b8ca05cf1348d19d5b8a6f857387b76f976eb759202c2adfce0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/21/be/d0436e3f1cf9bf38b9bb9b4a476399c77a1ab19f7172b45e19\n",
            "Successfully built gruut encodec bnnumerizer docopt gruut-ipa gruut_lang_de gruut_lang_en gruut_lang_es gruut_lang_fr\n",
            "Installing collected packages: sudachipy, jamo, hangul-romanize, gruut_lang_fr, gruut_lang_es, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, unidecode, sudachidict-core, python-crfsuite, pysbd, pypinyin, numpy, num2words, networkx, jsonlines, gruut-ipa, coqpit, anyascii, scipy, pandas, g2pkk, dateparser, contourpy, trainer, gruut, pynndescent, librosa, encodec, umap-learn, TTS\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.0\n",
            "    Uninstalling contourpy-1.3.0:\n",
            "      Successfully uninstalled contourpy-1.3.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.2.post1\n",
            "    Uninstalling librosa-0.10.2.post1:\n",
            "      Successfully uninstalled librosa-0.10.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.22.0 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "astropy 6.1.5 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "bigframes 1.25.0 requires numpy>=1.24.0, but you have numpy 1.22.0 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.22.0 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.14.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires numpy<3,>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.22.0 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.22.0 anyascii-0.3.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.7 contourpy-1.2.1 coqpit-0.0.17 dateparser-1.1.8 docopt-0.6.2 encodec-0.1.1 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.1 gruut_lang_en-2.0.1 gruut_lang_es-2.0.1 gruut_lang_fr-2.0.2 hangul-romanize-0.1.0 jamo-0.4.1 jsonlines-1.2.0 librosa-0.10.0 networkx-2.8.8 num2words-0.5.13 numpy-1.22.0 pandas-1.5.3 pynndescent-0.5.13 pypinyin-0.53.0 pysbd-0.3.4 python-crfsuite-0.9.11 scipy-1.11.4 sudachidict-core-20241021 sudachipy-0.6.8 trainer-0.0.36 umap-learn-0.5.7 unidecode-1.3.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f178c7906e024a8daf8580cdd98bc5e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mutagen\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/194.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mutagen\n",
            "Successfully installed mutagen-1.47.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.optim as optim\n",
        "from transformers import WavLMModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch import nn\n",
        "from google.colab import drive\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Import the GlowTTS config and model\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import multiprocessing\n",
        "multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "XW24YISitZ2M",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSL Encoder Implementation (WavLM Integration)"
      ],
      "metadata": {
        "id": "Cw5_3cUmif-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements the Self-Supervised Learning encoder component using WavLM-Large:\n",
        "\n",
        "### Key Features\n",
        "1. Model Initialization:\n",
        "   - Loads WavLM-Large model from HuggingFace\n",
        "   - Automatically selects GPU if available\n",
        "   - Sets model to evaluation mode\n",
        "\n",
        "2. Feature Extraction:\n",
        "   - Uses WavLM's 6th layer for optimal feature representation\n",
        "   - Handles automatic resampling to 16kHz\n",
        "   - Manages proper tensor dimensions and device placement\n",
        "   - Outputs 1024-dimensional feature vectors\n",
        "\n",
        "3. Audio Processing:\n",
        "   - Supports variable length inputs\n",
        "   - Handles mono/stereo conversion\n",
        "   - Implements automatic batching\n",
        "\n",
        "### Technical Details\n",
        "- Input: Audio waveform tensor [B, T] or [1, T]\n",
        "- Output: SSL features [B, T', 1024]\n",
        "- Uses @torch.no_grad() for efficient inference\n",
        "- Includes sample rate verification and conversion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lke3HJVbZDmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SSLEncoder:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        print(f\"Loading WavLM model to {device}...\")\n",
        "        self.model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device)\n",
        "        self.model.eval()\n",
        "        print(\"WavLM model loaded successfully!\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_features(self, waveform, sample_rate=16000):\n",
        "        \"\"\"Extract WavLM features from the 6th layer\"\"\"\n",
        "        # Resample if sample rate is not 16000 Hz\n",
        "        if sample_rate != 16000:\n",
        "            waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "        # Ensure waveform is properly batched\n",
        "        if waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        # Move waveform to the specified device\n",
        "        waveform = waveform.to(self.device)\n",
        "        outputs = self.model(waveform, output_hidden_states=True)\n",
        "\n",
        "        # Extract features from the 6th layer\n",
        "        features = outputs.hidden_states[6]\n",
        "        return features\n",
        "\n",
        "'''# Example usage\n",
        "ssl_encoder = SSLEncoder()\n",
        "\n",
        "# Load a sample audio file (replace 'path_to_audio_file.wav' with the actual file path)\n",
        "waveform, sample_rate = torchaudio.load('/content/harvard.wav')\n",
        "\n",
        "# Extract features\n",
        "features = ssl_encoder.extract_features(waveform, sample_rate)\n",
        "print(\"Extracted features shape:\", features.shape)'''\n"
      ],
      "metadata": {
        "id": "U3onB44pwWbx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cd08572c-1954-4847-9353-310b03b4215f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Example usage\\nssl_encoder = SSLEncoder()\\n\\n# Load a sample audio file (replace \\'path_to_audio_file.wav\\' with the actual file path)\\nwaveform, sample_rate = torchaudio.load(\\'/content/harvard.wav\\')\\n\\n# Extract features\\nfeatures = ssl_encoder.extract_features(waveform, sample_rate)\\nprint(\"Extracted features shape:\", features.shape)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-SSL Model (GlowTTS Adaptation)"
      ],
      "metadata": {
        "id": "1vjI_ouviak3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements the text-to-SSL conversion using a modified GlowTTS architecture:\n",
        "\n",
        "### Architecture Overview\n",
        "1. Configuration Setup:\n",
        "   - num_chars: 148 for English character set\n",
        "   - out_channels: 1024 to match WavLM features\n",
        "   - hidden_channels: 192 for encoder/decoder\n",
        "   - encoder_type: \"rel_pos_transformer\"\n",
        "\n",
        "2. Model Components:\n",
        "   - Transformer-based text encoder\n",
        "   - Duration predictor\n",
        "   - Flow-based decoder\n",
        "   - Speaker-independent design\n",
        "\n",
        "### Key Features\n",
        "- Non-autoregressive architecture\n",
        "- Flow-based feature generation\n",
        "- Duration prediction for proper alignment\n",
        "- Batch processing support\n",
        "- Device-agnostic implementation"
      ],
      "metadata": {
        "id": "h5C6svUUXpAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TEST TEST TEST TEST TEST TEST TEST TEST\n",
        "class TextToSSL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initialize GlowTTS config with modified output size\n",
        "        config = GlowTTSConfig(\n",
        "            num_chars=128,  # Standard English character set size\n",
        "            out_channels=1024,  # Match WavLM-Large output dimension\n",
        "        )\n",
        "\n",
        "        # Initialize GlowTTS model with modified config\n",
        "        self.glow_tts = GlowTTS(config)\n",
        "\n",
        "    def forward(self, text, text_lengths, y=None, gen=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the Text-to-SSL model\n",
        "\n",
        "        Args:\n",
        "            text (torch.Tensor): Text input tensor [B, T]\n",
        "            text_lengths (torch.Tensor): Length of each text sequence [B]\n",
        "            y (torch.Tensor, optional): Target SSL features [B, T, 1024]. Used only during training\n",
        "            gen (bool): Whether in generation mode or not\n",
        "\n",
        "        Returns:\n",
        "            During training (gen=False):\n",
        "                Dictionary containing:\n",
        "                - z: Transformed features\n",
        "                - z_m: Mean of transformed features\n",
        "                - z_logs: Log standard deviation of transformed features\n",
        "                - logdet: Log determinant of transformation\n",
        "                - z_mask: Mask for valid frames\n",
        "                - logw: Log durations\n",
        "                - logw_: Target log durations\n",
        "                - attn: Attention alignments\n",
        "            During inference (gen=True):\n",
        "                Dictionary containing:\n",
        "                - ssl_features: Generated SSL features\n",
        "                - attn: Attention alignments\n",
        "        \"\"\"\n",
        "        if gen:\n",
        "            return self.generate(text, text_lengths)\n",
        "\n",
        "        # Calculate y_lengths if y is provided\n",
        "        if y is not None:\n",
        "            y_lengths = torch.tensor([y.size(1)] * y.size(0), dtype=torch.long, device=y.device)\n",
        "        else:\n",
        "            y_lengths = None\n",
        "\n",
        "        # Process through GlowTTS\n",
        "        outputs = self.glow_tts(\n",
        "            text,\n",
        "            text_lengths,\n",
        "            y,\n",
        "            y_lengths,\n",
        "            aux_input={\"d_vectors\": None, \"speaker_ids\": None}\n",
        "        )\n",
        "\n",
        "        # Return all components needed for MLE loss\n",
        "        return {\n",
        "            'z': outputs.get('z', None),\n",
        "            'z_m': outputs.get('z_m', None),\n",
        "            'z_logs': outputs.get('z_logs', None),\n",
        "            'logdet': outputs.get('logdet', None),\n",
        "            'z_mask': outputs.get('z_mask', None),\n",
        "            'logw': outputs.get('logw', None),\n",
        "            'logw_': outputs.get('logw_', None),\n",
        "            'attn': outputs.get('alignments', None),\n",
        "            'ssl_features': outputs.get('y_mean', None)  # For compatibility with previous code\n",
        "        }\n",
        "\n",
        "    def generate(self, text, text_lengths):\n",
        "        \"\"\"\n",
        "        Generate SSL features from text input during inference\n",
        "        \"\"\"\n",
        "        outputs = self.glow_tts.inference(\n",
        "            text,\n",
        "            aux_input={\"x_lengths\": text_lengths}\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ssl_features': outputs['model_outputs'],\n",
        "            'attn': outputs['alignments']\n",
        "        }"
      ],
      "metadata": {
        "id": "2DdIi5s1zTfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextToSSL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize GlowTTS config with modified output size\n",
        "        config = GlowTTSConfig(\n",
        "            num_chars=128,  # Standard English character set size\n",
        "            out_channels=1024,  # Match WavLM-Large output dimension\n",
        "        ) # Mostly default configs\n",
        "\n",
        "        # Initialize GlowTTS model with modified config\n",
        "        self.glow_tts = GlowTTS(config)\n",
        "\n",
        "    def forward(self, text, text_lengths, y=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the Text-to-SSL model\n",
        "\n",
        "        Args:\n",
        "            text (torch.Tensor): Text input tensor [B, T]\n",
        "            text_lengths (torch.Tensor): Length of each text sequence [B]\n",
        "            y (torch.Tensor, optional): Target SSL features [B, T, 1024]. Used only during training.\n",
        "\n",
        "        Returns:\n",
        "            Dict containing:\n",
        "                - ssl_features: Predicted SSL features [B, T, 1024]\n",
        "                - alignments: Alignment matrix between text and features\n",
        "                - durations_log: Log durations for each input token\n",
        "        \"\"\"\n",
        "        if y is not None:\n",
        "            y_lengths = torch.tensor([y.size(1)] * y.size(0), dtype=torch.long).to(y.device)  # Calculate lengths of y\n",
        "        else:\n",
        "            y_lengths = None\n",
        "\n",
        "        # Process through GlowTTS\n",
        "        outputs = self.glow_tts(\n",
        "            text,\n",
        "            text_lengths,\n",
        "            y,  # Pass the target features\n",
        "            y_lengths,  # Pass the lengths of the target features\n",
        "            aux_input={\"d_vectors\": None, \"speaker_ids\": None}\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"ssl_features\": outputs[\"y_mean\"],  # [B, T, 1024]\n",
        "            \"alignments\": outputs[\"alignments\"],\n",
        "            \"durations_log\": outputs[\"durations_log\"]\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self, text, text_lengths):\n",
        "        \"\"\"\n",
        "        Generate SSL features from text input during inference\n",
        "\n",
        "        Args:\n",
        "            text (torch.Tensor): Text input tensor [B, T]\n",
        "            text_lengths (torch.Tensor): Length of each text sequence [B]\n",
        "\n",
        "        Returns:\n",
        "            Dict containing:\n",
        "                - ssl_features: Generated SSL features [B, T, 1024]\n",
        "                - alignments: Alignment matrix between text and features\n",
        "        \"\"\"\n",
        "        outputs = self.glow_tts.inference(\n",
        "            text,\n",
        "            aux_input={\"x_lengths\": text_lengths}\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"ssl_features\": outputs[\"model_outputs\"],  # [B, T, 1024]\n",
        "            \"alignments\": outputs[\"alignments\"]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "9W4Jza3AYIYB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GlowwTTS Training"
      ],
      "metadata": {
        "id": "rvMN1tiTqa9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the LJSpeech dataset\n",
        "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "\n",
        "# Extract the dataset\n",
        "!tar -xjf LJSpeech-1.1.tar.bz2\n",
        "\n",
        "# Verify the extraction by listing the contents\n",
        "!ls LJSpeech-1.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s-CY8TcqfFs",
        "outputId": "12fd1b74-1dc5-4c74-d2d2-4a21e3fa00b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-13 22:39:27--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "Resolving data.keithito.com (data.keithito.com)... 24.199.73.137\n",
            "Connecting to data.keithito.com (data.keithito.com)|24.199.73.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2748572632 (2.6G) [text/plain]\n",
            "Saving to: ‘LJSpeech-1.1.tar.bz2’\n",
            "\n",
            "LJSpeech-1.1.tar.bz 100%[===================>]   2.56G   159MB/s    in 23s     \n",
            "\n",
            "2024-11-13 22:39:50 (116 MB/s) - ‘LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n",
            "\n",
            "metadata.csv  README  wavs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Prep / Pre-process"
      ],
      "metadata": {
        "id": "5HdhdjfVDc5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up the data processing pipeline for training SSL-TTS. It performs three key operations:\n",
        "\n",
        "1. **Audio Loading and Resampling**\n",
        "   - Loads audio files from LJSpeech dataset\n",
        "   - Resamples them from 22.05kHz to 16kHz (required by WavLM)\n",
        "\n",
        "2. **Feature Extraction**\n",
        "   - Uses WavLM to convert raw audio into high-level speech features\n",
        "   - Instead of using mel-spectrograms, we get 1024-dimensional WavLM features\n",
        "   - These features contain rich information about speech content and speaker characteristics\n",
        "\n",
        "3. **Batch Processing**\n",
        "   - Handles variable-length audio files by padding them to the same length\n",
        "   - Creates batches of features and their corresponding text transcriptions\n",
        "   - Makes the data ready for training the GlowTTS model\n",
        "\n",
        "This pipeline transforms raw audio into the format needed for training our SSL-TTS system, where GlowTTS will learn to predict WavLM features from text."
      ],
      "metadata": {
        "id": "FE3hQCzyTGKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class for LJSpeech with resampling and feature extraction\n",
        "class LJSpeechDataset(Dataset):\n",
        "    def __init__(self, root_dir, metadata_file, ssl_encoder, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.metadata = pd.read_csv(metadata_file, sep=\"|\", header=None, names=[\"file\", \"text\", \"normalized_text\"])\n",
        "        self.resampler = torchaudio.transforms.Resample(orig_freq=22050, new_freq=16000)\n",
        "        self.ssl_encoder = ssl_encoder # WavLM\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get the audio file path\n",
        "        wav_file = os.path.join(self.root_dir, \"wavs\", self.metadata.iloc[idx, 0] + \".wav\")\n",
        "\n",
        "        # Load the audio file\n",
        "        waveform, sample_rate = torchaudio.load(wav_file)\n",
        "\n",
        "        # Resample the audio to 16 kHz if needed\n",
        "        if sample_rate != 16000:\n",
        "            waveform = self.resampler(waveform)\n",
        "\n",
        "        # Extract WavLM features\n",
        "        features = self.ssl_encoder.extract_features(waveform)\n",
        "\n",
        "        # Get the corresponding text\n",
        "        text = self.metadata.iloc[idx, 1]\n",
        "\n",
        "        # Apply any specified transformations\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return features, text  # Return WavLM features and the text\n",
        "\n",
        "# Custom collate function to handle variable-length waveforms\n",
        "def collate_fn(batch):\n",
        "    # Separate the components of the batch\n",
        "    features, texts = zip(*batch)\n",
        "\n",
        "    # Find the maximum length in the current batch\n",
        "    max_length = max(feature.size(1) for feature in features)\n",
        "\n",
        "    # Pad the features to the maximum length and remove the extra channel dimension\n",
        "    padded_features = [torch.nn.functional.pad(feature.squeeze(0), (0, 0, 0, max_length - feature.size(1))) for feature in features]\n",
        "\n",
        "    # Stack the features into a single tensor\n",
        "    features_tensor = torch.stack(padded_features)\n",
        "\n",
        "    return features_tensor, texts\n",
        "\n",
        "# Initialize the SSLEncoder\n",
        "ssl_encoder = SSLEncoder()\n",
        "\n",
        "# Directory and file paths\n",
        "root_dir = \"LJSpeech-1.1\"\n",
        "metadata_file = os.path.join(root_dir, \"metadata.csv\")\n",
        "\n",
        "# Initialize the dataset and data loader\n",
        "dataset = LJSpeechDataset(root_dir, metadata_file, ssl_encoder)\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "# Example: Iterate through the data loader and print a sample\n",
        "for features, text in data_loader:\n",
        "    print(\"Features shape:\", features.shape)\n",
        "    print(\"Text:\", text)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "0da9cfb5749f4e7ea5929607e88f531f",
            "2108d706787340ab8aed388551a5f74a",
            "36ef3a731a024bfaa78d55a662f66d18",
            "41080295b27d4198988ec75f0a7d2a96",
            "6e9e0cde29dc41f7b4b4a8bc458462fa",
            "5ab799ab2d474ea19bbe1e69287483a0",
            "83d9647b3aaf4cbd84da51b6ca14ea40",
            "1eb1f75847324e1da58b384bb9d15f1d",
            "4e21d17069c64761b422c4306121f6c9",
            "80d505bd20a547eda1d0e5c1d523c9b0",
            "d1e48faffbbf472497a304015816e4d1",
            "4c2ab7f4517e4a938a644572205f4a6a",
            "82b5d76b096e4534a7cfeec52e2e3991",
            "5ad8d4b101664f9ba1bebb44c77c36b4",
            "52913fa174a345dd9db79bbd91982889",
            "787423274d984c5b8cc9e1cb33ddf16a",
            "bcdacc192df44791b56d6fdfbde1e309",
            "6cb9b5b119ee4e55b98e766eb9198298",
            "2b39f65cb09c49a0ad46492f370a96ca",
            "0328842655fc4c3bbf1210430e356b6d",
            "4c993d9990f448afbd9ed178f8a2ecf1",
            "360d03e929d04fddb4432def94ee8e22",
            "afff6630b3d5447093e4c73fb62d9e01",
            "10ddc70b280c4ba083f148c1d0c722da",
            "7c8fd9555295460fbcbd21ef7089a3d8",
            "0c661e736d7c4cb78bbac2c0916ef5ea",
            "d9f7c8b5a72c47529ca7f5fc7873a68d",
            "6825e80446a1425898647160ee7fe274",
            "1175930ebb7a40ef80be881593f0a5ee",
            "8930060eca964740ad369b87c83d18d0",
            "6994a48630fd4ab89b387376986ce5a5",
            "781234e9db61401096ef22919c683c9c",
            "c78042474eda40f48d6f04c4c25ca687"
          ]
        },
        "id": "zuei0wDPtjiG",
        "outputId": "88432103-18af-4722-a524-ab5e45587120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WavLM model to cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0da9cfb5749f4e7ea5929607e88f531f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c2ab7f4517e4a938a644572205f4a6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WavLM model loaded successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afff6630b3d5447093e4c73fb62d9e01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: torch.Size([4, 414, 1024])\n",
            "Text: ('The Bureau had no earlier information suggesting that Oswald had left the United States.', 'according to the discretion of the court before whom the prisoners might be tried.', 'Agent Fain retired from the FBI in October 1962, and the closed Oswald case was not reassigned.', 'could not be carried out till then. It is to be feared that long after the opening of White Cross Street prison,')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3)} GB\")"
      ],
      "metadata": {
        "id": "c8U8JR886rgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Cell 1\n",
        "Saves only on runtime, no checkpoints."
      ],
      "metadata": {
        "id": "3nnQT3ktDXHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TextToSSL model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextToSSL().to(device=device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with a fixed number of steps\n",
        "num_steps = 650000  # Total number of steps\n",
        "current_step = 0\n",
        "\n",
        "for epoch in range(1000000):  # Large number to ensure we cover all steps\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0  # Reset running loss at the start of each epoch\n",
        "\n",
        "    for features, text in data_loader:\n",
        "        if current_step >= num_steps:\n",
        "            break\n",
        "\n",
        "        # Move features to device\n",
        "        features = features.to(device=device)\n",
        "\n",
        "        # Convert text to tensor (pad sequences to the same length)\n",
        "\n",
        "        max_length = max(len(t) for t in text)\n",
        "        text_tensor = torch.tensor(\n",
        "            [[ord(c) for c in t.ljust(max_length)] for t in text],  # Pad with spaces (or any padding character)\n",
        "            dtype=torch.long\n",
        "        ).to(device=device)\n",
        "\n",
        "        max_index = 128 - 1\n",
        "        text_tensor = torch.clamp(text_tensor, max=max_index)\n",
        "\n",
        "        # Calculate text lengths\n",
        "        text_lengths = torch.tensor([len(t) for t in text], dtype=torch.long).to(device=device)\n",
        "\n",
        "\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(text_tensor, text_lengths, features)  # Pass features as the target for training\n",
        "\n",
        "        # Adjust for sequence length mismatch\n",
        "        predicted_features = outputs[\"ssl_features\"]\n",
        "        target_length = features.size(1)\n",
        "        predicted_length = predicted_features.size(1)\n",
        "\n",
        "        if predicted_length > target_length:\n",
        "            # Truncate the predicted features\n",
        "            predicted_features = predicted_features[:, :target_length, :]\n",
        "        elif predicted_length < target_length:\n",
        "            # Truncate the target features\n",
        "            features = features[:, :predicted_length, :]\n",
        "\n",
        "        # Compute the loss between the predicted and actual WavLM features\n",
        "        loss = criterion(predicted_features, features)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update the model parameters\n",
        "\n",
        "        running_loss += loss.item()  # Accumulate the loss\n",
        "        current_step += 1  # Increment the step count\n",
        "\n",
        "        if current_step % 100 == 0:\n",
        "            print(f\"Step {current_step}/{num_steps}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Calculate the average loss for the epoch\n",
        "    average_loss = running_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {average_loss}\")\n",
        "\n",
        "    if current_step >= num_steps:\n",
        "        break\n",
        "\n",
        "print(\"Training completed!\")\n",
        "torch.save(model.state_dict(), 'GlowTTS_state.pth')"
      ],
      "metadata": {
        "id": "KmeOcSv600QI",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea5de66-9817-48f8-8eba-41de9e1257ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100/650000, Loss: 10.282491683959961\n",
            "Step 200/650000, Loss: 9.229987144470215\n",
            "Step 300/650000, Loss: 10.324702262878418\n",
            "Step 400/650000, Loss: 10.808058738708496\n",
            "Step 500/650000, Loss: 11.408522605895996\n",
            "Step 600/650000, Loss: 11.445104598999023\n",
            "Step 700/650000, Loss: 8.919713973999023\n",
            "Step 800/650000, Loss: 11.184569358825684\n",
            "Step 900/650000, Loss: 8.18460750579834\n",
            "Step 1000/650000, Loss: 10.558945655822754\n",
            "Step 1100/650000, Loss: 11.150132179260254\n",
            "Step 1200/650000, Loss: 9.41183853149414\n",
            "Step 1300/650000, Loss: 9.975044250488281\n",
            "Step 1400/650000, Loss: 11.638545989990234\n",
            "Step 1500/650000, Loss: 12.243314743041992\n",
            "Step 1600/650000, Loss: 10.34382438659668\n",
            "Step 1700/650000, Loss: 11.027650833129883\n",
            "Step 1800/650000, Loss: 10.62975025177002\n",
            "Step 1900/650000, Loss: 9.639217376708984\n",
            "Step 2000/650000, Loss: 12.009521484375\n",
            "Step 2100/650000, Loss: 11.1110200881958\n",
            "Step 2200/650000, Loss: 8.596921920776367\n",
            "Step 2300/650000, Loss: 11.377479553222656\n",
            "Step 2400/650000, Loss: 9.77747917175293\n",
            "Step 2500/650000, Loss: 8.410871505737305\n",
            "Step 2600/650000, Loss: 10.581392288208008\n",
            "Step 2700/650000, Loss: 6.705504417419434\n",
            "Step 2800/650000, Loss: 10.317279815673828\n",
            "Step 2900/650000, Loss: 10.555107116699219\n",
            "Step 3000/650000, Loss: 11.350861549377441\n",
            "Step 3100/650000, Loss: 11.702925682067871\n",
            "Step 3200/650000, Loss: 9.534330368041992\n",
            "Epoch 1, Average Loss: 10.355648028541157\n",
            "Step 3300/650000, Loss: 9.911284446716309\n",
            "Step 3400/650000, Loss: 7.8041582107543945\n",
            "Step 3500/650000, Loss: 9.413936614990234\n",
            "Step 3600/650000, Loss: 9.488840103149414\n",
            "Step 3700/650000, Loss: 11.053956985473633\n",
            "Step 3800/650000, Loss: 9.69921875\n",
            "Step 3900/650000, Loss: 8.793322563171387\n",
            "Step 4000/650000, Loss: 11.044032096862793\n",
            "Step 4100/650000, Loss: 11.13070011138916\n",
            "Step 4200/650000, Loss: 12.206740379333496\n",
            "Step 4300/650000, Loss: 11.257468223571777\n",
            "Step 4400/650000, Loss: 10.247432708740234\n",
            "Step 4500/650000, Loss: 9.228822708129883\n",
            "Step 4600/650000, Loss: 8.787064552307129\n",
            "Step 4700/650000, Loss: 7.700798034667969\n",
            "Step 4800/650000, Loss: 11.141498565673828\n",
            "Step 4900/650000, Loss: 10.508011817932129\n",
            "Step 5000/650000, Loss: 9.745408058166504\n",
            "Step 5100/650000, Loss: 10.652911186218262\n",
            "Step 5200/650000, Loss: 11.230945587158203\n",
            "Step 5300/650000, Loss: 9.503168106079102\n",
            "Step 5400/650000, Loss: 10.562902450561523\n",
            "Step 5500/650000, Loss: 9.934212684631348\n",
            "Step 5600/650000, Loss: 9.788484573364258\n",
            "Step 5700/650000, Loss: 9.324376106262207\n",
            "Step 5800/650000, Loss: 10.15325927734375\n",
            "Step 5900/650000, Loss: 9.186380386352539\n",
            "Step 6000/650000, Loss: 10.938871383666992\n",
            "Step 6100/650000, Loss: 8.858135223388672\n",
            "Step 6200/650000, Loss: 9.189379692077637\n",
            "Step 6300/650000, Loss: 9.217663764953613\n",
            "Step 6400/650000, Loss: 8.987669944763184\n",
            "Step 6500/650000, Loss: 10.983940124511719\n",
            "Epoch 2, Average Loss: 9.94351675921724\n",
            "Step 6600/650000, Loss: 9.774358749389648\n",
            "Step 6700/650000, Loss: 8.58397388458252\n",
            "Step 6800/650000, Loss: 8.54182243347168\n",
            "Step 6900/650000, Loss: 9.940564155578613\n",
            "Step 7000/650000, Loss: 9.333863258361816\n",
            "Step 7100/650000, Loss: 10.234789848327637\n",
            "Step 7200/650000, Loss: 9.435725212097168\n",
            "Step 7300/650000, Loss: 8.867859840393066\n",
            "Step 7400/650000, Loss: 10.085351943969727\n",
            "Step 7500/650000, Loss: 10.578180313110352\n",
            "Step 7600/650000, Loss: 9.93752384185791\n",
            "Step 7700/650000, Loss: 10.150619506835938\n",
            "Step 7800/650000, Loss: 11.342763900756836\n",
            "Step 7900/650000, Loss: 9.733967781066895\n",
            "Step 8000/650000, Loss: 10.59369945526123\n",
            "Step 8100/650000, Loss: 9.012063026428223\n",
            "Step 8200/650000, Loss: 12.201945304870605\n",
            "Step 8300/650000, Loss: 9.949189186096191\n",
            "Step 8400/650000, Loss: 7.602894306182861\n",
            "Step 8500/650000, Loss: 11.586156845092773\n",
            "Step 8600/650000, Loss: 11.53203010559082\n",
            "Step 8700/650000, Loss: 9.20850944519043\n",
            "Step 8800/650000, Loss: 9.730679512023926\n",
            "Step 8900/650000, Loss: 9.849823951721191\n",
            "Step 9000/650000, Loss: 9.429506301879883\n",
            "Step 9100/650000, Loss: 10.410286903381348\n",
            "Step 9200/650000, Loss: 9.946516990661621\n",
            "Step 9300/650000, Loss: 10.463889122009277\n",
            "Step 9400/650000, Loss: 10.253073692321777\n",
            "Step 9500/650000, Loss: 9.983466148376465\n",
            "Step 9600/650000, Loss: 10.659760475158691\n",
            "Step 9700/650000, Loss: 10.3414306640625\n",
            "Step 9800/650000, Loss: 10.178089141845703\n",
            "Epoch 3, Average Loss: 10.289897654875544\n",
            "Step 9900/650000, Loss: 10.37836742401123\n",
            "Step 10000/650000, Loss: 10.313926696777344\n",
            "Step 10100/650000, Loss: 10.677473068237305\n",
            "Step 10200/650000, Loss: 10.59854507446289\n",
            "Step 10300/650000, Loss: 8.101556777954102\n",
            "Step 10400/650000, Loss: 12.363823890686035\n",
            "Step 10500/650000, Loss: 7.544560432434082\n",
            "Step 10600/650000, Loss: 9.997477531433105\n",
            "Step 10700/650000, Loss: 11.568087577819824\n",
            "Step 10800/650000, Loss: 10.398776054382324\n",
            "Step 10900/650000, Loss: 10.599241256713867\n",
            "Step 11000/650000, Loss: 8.946595191955566\n",
            "Step 11100/650000, Loss: 8.579252243041992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Cell 2"
      ],
      "metadata": {
        "id": "6WRU5uMbMQp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoints and saves on google drive. It also can start from somewhere, it'll grap the file from google drive and start from there (see on driver cell below)."
      ],
      "metadata": {
        "id": "R8L9Ss2fZhxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckpointManager:\n",
        "    def __init__(self, model, optimizer, save_dir, save_interval=10000):\n",
        "        \"\"\"\n",
        "        Initialize checkpoint manager\n",
        "        Args:\n",
        "            model: The model to save\n",
        "            optimizer: The optimizer to save\n",
        "            save_dir: Directory path in Google Drive to save checkpoints\n",
        "            save_interval: Save checkpoint every N steps\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.save_dir = save_dir\n",
        "        self.save_interval = save_interval\n",
        "\n",
        "        # Create save directory if it doesn't exist\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize training history\n",
        "        self.history = {\n",
        "            'steps': [],\n",
        "            'losses': [],\n",
        "            'timestamp': []\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, step, loss):\n",
        "        \"\"\"Save checkpoint and update history\"\"\"\n",
        "        if step % self.save_interval == 0:\n",
        "            # Create checkpoint filename with timestamp\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            filename = f'GlowTTS_checkpoint_step_{step}_{timestamp}.pth'\n",
        "            filepath = os.path.join(self.save_dir, filename)\n",
        "\n",
        "            # Save checkpoint with all necessary information\n",
        "            checkpoint = {\n",
        "                'step': step,\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Save checkpoint\n",
        "                torch.save(checkpoint, filepath)\n",
        "                print(f\"\\nCheckpoint saved successfully at step {step}: {filepath}\")\n",
        "\n",
        "                # Update history\n",
        "                self.history['steps'].append(step)\n",
        "                self.history['losses'].append(loss)\n",
        "                self.history['timestamp'].append(timestamp)\n",
        "\n",
        "                # Save history to JSON\n",
        "                history_file = os.path.join(self.save_dir, 'training_history.json')\n",
        "                with open(history_file, 'w') as f:\n",
        "                    json.dump(self.history, f, indent=4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError saving checkpoint at step {step}: {str(e)}\")\n",
        "\n",
        "def train_model_with_checkpoints(model, data_loader, save_dir, num_steps=650000, save_interval=10000,\n",
        "                               device=None, resume_from=None):\n",
        "\n",
        "   ''' # Mount Google Drive\n",
        "    drive.mount('/content/drive')'''\n",
        "    \"\"\"\n",
        "    Training function with optimizer settings matching the Glow-TTS paper\n",
        "    \"\"\"\n",
        "    # Setup device\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device=device)\n",
        "\n",
        "    # Initialize optimizer with RAdam as used in paper\n",
        "    from torch.optim import RAdam\n",
        "    optimizer = RAdam(\n",
        "        model.parameters(),\n",
        "        lr=1e-3,\n",
        "        betas=(0.9, 0.998),\n",
        "        eps=1e-9,\n",
        "        weight_decay=1e-6\n",
        "    )\n",
        "\n",
        "    # Noam learning rate scheduler\n",
        "    def noam_learning_rate_decay(init_lr, global_step, warmup_steps=4000):\n",
        "        step = global_step + 1.\n",
        "        lr = init_lr * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
        "        return lr\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    current_step = 0\n",
        "    best_loss = float('inf')\n",
        "    running_loss = 0.0\n",
        "    loss_window = []\n",
        "\n",
        "    # Resume from checkpoint if specified\n",
        "    if resume_from is not None:\n",
        "        print(f\"Loading checkpoint from {resume_from}\")\n",
        "        checkpoint = torch.load(resume_from)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        current_step = checkpoint['step']\n",
        "        if 'best_loss' in checkpoint:\n",
        "            best_loss = checkpoint['best_loss']\n",
        "        print(f\"Resumed from step {current_step}\")\n",
        "\n",
        "    # Initialize checkpoint manager\n",
        "    checkpoint_manager = CheckpointManager(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        save_dir=save_dir,\n",
        "        save_interval=save_interval\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(1000000):\n",
        "        for features, text in data_loader:\n",
        "            if current_step >= num_steps:\n",
        "                break\n",
        "\n",
        "            # Adjust learning rate using Noam decay\n",
        "            lr = noam_learning_rate_decay(1e-3, current_step)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "            # Process inputs\n",
        "            features = features.to(device=device)\n",
        "\n",
        "            # Convert text to tensor\n",
        "            max_length = max(len(t) for t in text)\n",
        "            text_tensor = torch.tensor(\n",
        "                [[ord(c) for c in t.ljust(max_length)] for t in text],\n",
        "                dtype=torch.long\n",
        "            ).to(device=device)\n",
        "\n",
        "            max_index = 128 - 1\n",
        "            text_tensor = torch.clamp(text_tensor, max=max_index)\n",
        "\n",
        "            text_lengths = torch.tensor([len(t) for t in text], dtype=torch.long).to(device=device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_tensor, text_lengths, features)\n",
        "\n",
        "            # Handle sequence length mismatch\n",
        "            predicted_features = outputs[\"ssl_features\"]\n",
        "            target_length = features.size(1)\n",
        "            predicted_length = predicted_features.size(1)\n",
        "\n",
        "            if predicted_length > target_length:\n",
        "                predicted_features = predicted_features[:, :target_length, :]\n",
        "            elif predicted_length < target_length:\n",
        "                features = features[:, :predicted_length, :]\n",
        "\n",
        "            # Compute loss - try to use negative log-likelihood if available\n",
        "            if \"log_likelihood\" in outputs:\n",
        "                loss = -outputs[\"log_likelihood\"].mean()\n",
        "            else:\n",
        "                # Scale MSE loss to be in similar range as NLL would be\n",
        "                mse_loss = torch.nn.functional.mse_loss(predicted_features, features)\n",
        "                loss = mse_loss * 1000  # Scale factor might need tuning\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(\"Warning: NaN loss detected\")\n",
        "                continue\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "            # Skip step if gradient norm is too high\n",
        "            if grad_norm > 1000:\n",
        "                print(f\"Warning: High gradient norm {grad_norm}, skipping step\")\n",
        "                continue\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update tracking\n",
        "            running_loss += loss.item()\n",
        "            loss_window.append(loss.item())\n",
        "            if len(loss_window) > 100:\n",
        "                loss_window.pop(0)\n",
        "\n",
        "            current_step += 1\n",
        "\n",
        "            # Logging\n",
        "            if current_step % 250 == 0:\n",
        "                moving_avg_loss = sum(loss_window) / len(loss_window)\n",
        "                print(f\"Step {current_step}/{num_steps}, \"\n",
        "                      f\"Loss: {loss.item():.4f}, \"\n",
        "                      f\"Moving Avg Loss: {moving_avg_loss:.4f}, \"\n",
        "                      f\"Gradient Norm: {grad_norm:.2f}, \"\n",
        "                      f\"LR: {lr:.6f}\")\n",
        "\n",
        "                checkpoint_manager.save_checkpoint(current_step, loss.item())\n",
        "\n",
        "                # Save best model\n",
        "                if moving_avg_loss < best_loss:\n",
        "                    best_loss = moving_avg_loss\n",
        "                    best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
        "                    torch.save({\n",
        "                        'step': current_step,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'loss': best_loss\n",
        "                    }, best_model_path)\n",
        "\n",
        "        # Epoch metrics\n",
        "        average_loss = running_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {average_loss:.6f}\")\n",
        "        running_loss = 0.0\n",
        "\n",
        "        if current_step >= num_steps:\n",
        "            break\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    final_path = os.path.join(save_dir, 'GlowTTS_final_model.pth')\n",
        "    torch.save({\n",
        "        'step': current_step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': average_loss\n",
        "    }, final_path)\n"
      ],
      "metadata": {
        "id": "SnG4VvwJI3L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckpointManager:\n",
        "    def __init__(self, model, optimizer, save_dir, save_interval=10000):\n",
        "        \"\"\"\n",
        "        Initialize checkpoint manager\n",
        "        Args:\n",
        "            model: The model to save\n",
        "            optimizer: The optimizer to save\n",
        "            save_dir: Directory path in Google Drive to save checkpoints\n",
        "            save_interval: Save checkpoint every N steps\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.save_dir = save_dir\n",
        "        self.save_interval = save_interval\n",
        "\n",
        "        # Create save directory if it doesn't exist\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize training history\n",
        "        self.history = {\n",
        "            'steps': [],\n",
        "            'losses': [],\n",
        "            'timestamp': []\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, step, loss):\n",
        "        \"\"\"Save checkpoint and update history\"\"\"\n",
        "        if step % self.save_interval == 0:\n",
        "            # Create checkpoint filename with timestamp\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            filename = f'GlowTTS_checkpoint_step_{step}_{timestamp}.pth'\n",
        "            filepath = os.path.join(self.save_dir, filename)\n",
        "\n",
        "            # Save checkpoint with all necessary information\n",
        "            checkpoint = {\n",
        "                'step': step,\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Save checkpoint\n",
        "                torch.save(checkpoint, filepath)\n",
        "                print(f\"\\nCheckpoint saved successfully at step {step}: {filepath}\")\n",
        "\n",
        "                # Update history\n",
        "                self.history['steps'].append(step)\n",
        "                self.history['losses'].append(loss)\n",
        "                self.history['timestamp'].append(timestamp)\n",
        "\n",
        "                # Save history to JSON\n",
        "                history_file = os.path.join(self.save_dir, 'training_history.json')\n",
        "                with open(history_file, 'w') as f:\n",
        "                    json.dump(self.history, f, indent=4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError saving checkpoint at step {step}: {str(e)}\")\n",
        "\n",
        "def train_model_with_checkpoints(model, data_loader, save_dir, num_steps=650000, save_interval=10000,\n",
        "                               device=None, resume_from=None):\n",
        "    \"\"\"Training function with safe attention handling and gradient scaling\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device=device)\n",
        "\n",
        "    # Initialize optimizer with RAdam\n",
        "    from torch.optim import RAdam\n",
        "    optimizer = RAdam(\n",
        "        model.parameters(),\n",
        "        lr=0.001,\n",
        "        betas=(0.9, 0.998),\n",
        "        eps=1e-9,\n",
        "        weight_decay=1e-6\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    from torch.optim.lr_scheduler import StepLR\n",
        "    scheduler = StepLR(optimizer, step_size=50000, gamma=0.5)\n",
        "\n",
        "    def compute_loss(outputs, features, text_lengths):\n",
        "        \"\"\"Compute loss with safe handling of all components\"\"\"\n",
        "        predicted_features = outputs[\"ssl_features\"]\n",
        "        z = outputs.get('z')\n",
        "        z_m = outputs.get('z_m')\n",
        "        z_logs = outputs.get('z_logs')\n",
        "        logdet = outputs.get('logdet')\n",
        "        z_mask = outputs.get('z_mask')\n",
        "\n",
        "        # Basic reconstruction loss\n",
        "        target_length = features.size(1)\n",
        "        predicted_length = predicted_features.size(1)\n",
        "\n",
        "        if predicted_length > target_length:\n",
        "            predicted_features = predicted_features[:, :target_length, :]\n",
        "        elif predicted_length < target_length:\n",
        "            features = features[:, :predicted_length, :]\n",
        "\n",
        "        # Compute masked reconstruction loss\n",
        "        recon_loss = F.mse_loss(predicted_features, features)\n",
        "\n",
        "        # Initialize total loss with reconstruction loss\n",
        "        total_loss = recon_loss * 10.0  # Scale factor for stability\n",
        "\n",
        "        # Add flow loss components if available\n",
        "        if all(x is not None for x in [z, z_m, z_logs]):\n",
        "            # Normalize z_logs for stability\n",
        "            z_logs = torch.clamp(z_logs, min=-30, max=30)\n",
        "\n",
        "            # Compute flow loss\n",
        "            if z_mask is not None:\n",
        "                # Masked flow loss\n",
        "                flow_loss = torch.sum(z_logs + 0.5 * torch.exp(-2 * z_logs) * ((z - z_m) ** 2))\n",
        "                flow_loss = flow_loss / torch.sum(z_mask)  # Normalize by mask sum\n",
        "            else:\n",
        "                # Unmasked flow loss\n",
        "                flow_loss = torch.mean(z_logs + 0.5 * torch.exp(-2 * z_logs) * ((z - z_m) ** 2))\n",
        "\n",
        "            # Add logdet if available\n",
        "            if logdet is not None:\n",
        "                flow_loss = flow_loss - torch.mean(logdet)\n",
        "\n",
        "            total_loss = total_loss + flow_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    current_step = 0\n",
        "    best_loss = float('inf')\n",
        "    running_loss = 0.0\n",
        "    loss_window = []\n",
        "\n",
        "    # Resume from checkpoint if specified\n",
        "    if resume_from is not None:\n",
        "        print(f\"Loading checkpoint from {resume_from}\")\n",
        "        checkpoint = torch.load(resume_from)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        current_step = checkpoint['step']\n",
        "        if 'best_loss' in checkpoint:\n",
        "            best_loss = checkpoint['best_loss']\n",
        "        print(f\"Resumed from step {current_step}\")\n",
        "\n",
        "    # Initialize checkpoint manager\n",
        "    checkpoint_manager = CheckpointManager(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        save_dir=save_dir,\n",
        "        save_interval=save_interval\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(1000000):\n",
        "        for features, text in data_loader:\n",
        "            if current_step >= num_steps:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Process inputs\n",
        "                features = features.to(device=device)\n",
        "\n",
        "                # Convert text to tensor\n",
        "                max_length = max(len(t) for t in text)\n",
        "                text_tensor = torch.tensor(\n",
        "                    [[ord(c) for c in t.ljust(max_length)] for t in text],\n",
        "                    dtype=torch.long\n",
        "                ).to(device=device)\n",
        "\n",
        "                max_index = 128 - 1\n",
        "                text_tensor = torch.clamp(text_tensor, max=max_index)\n",
        "                text_lengths = torch.tensor([len(t) for t in text], dtype=torch.long).to(device=device)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Print debug info at first step\n",
        "                if current_step == 0:\n",
        "                    print(\"Features shape:\", features.shape)\n",
        "                    print(\"Text tensor shape:\", text_tensor.shape)\n",
        "                    print(\"Text lengths:\", text_lengths)\n",
        "\n",
        "                outputs = model(text_tensor, text_lengths, features, gen=False)\n",
        "\n",
        "                # Print output keys for debugging (only first step)\n",
        "                if current_step == 0:\n",
        "                    print(\"Available output keys:\", outputs.keys())\n",
        "                    for k, v in outputs.items():\n",
        "                        if v is not None:\n",
        "                            print(f\"{k} shape: {v.shape if hasattr(v, 'shape') else None}\")\n",
        "\n",
        "                # Compute loss\n",
        "                loss = compute_loss(outputs, features, text_lengths)\n",
        "\n",
        "                if torch.isnan(loss):\n",
        "                    print(\"Warning: NaN loss detected, skipping batch\")\n",
        "                    continue\n",
        "\n",
        "                # Scale loss for better gradient flow\n",
        "                loss = loss / 100.0  # Scale down for stability\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Skip step if gradient norm is too high\n",
        "                if grad_norm > 100:\n",
        "                    print(f\"Warning: High gradient norm {grad_norm}, skipping step\")\n",
        "                    continue\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                # Update tracking\n",
        "                running_loss += loss.item()\n",
        "                loss_window.append(loss.item())\n",
        "                if len(loss_window) > 100:\n",
        "                    loss_window.pop(0)\n",
        "\n",
        "                current_step += 1\n",
        "\n",
        "                # Logging\n",
        "                if current_step % 250 == 0:\n",
        "                    moving_avg_loss = sum(loss_window) / len(loss_window)\n",
        "                    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "                    print(f\"Step {current_step}/{num_steps}, \"\n",
        "                          f\"Loss: {loss.item():.4f}, \"\n",
        "                          f\"Moving Avg Loss: {moving_avg_loss:.4f}, \"\n",
        "                          f\"LR: {current_lr:.6f}, \"\n",
        "                          f\"Grad Norm: {grad_norm:.2f}\")\n",
        "\n",
        "                    checkpoint_manager.save_checkpoint(current_step, loss.item())\n",
        "\n",
        "                    # Save best model\n",
        "                    if moving_avg_loss < best_loss:\n",
        "                        best_loss = moving_avg_loss\n",
        "                        best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
        "                        torch.save({\n",
        "                            'step': current_step,\n",
        "                            'model_state_dict': model.state_dict(),\n",
        "                            'optimizer_state_dict': optimizer.state_dict(),\n",
        "                            'scheduler_state_dict': scheduler.state_dict(),\n",
        "                            'loss': best_loss\n",
        "                        }, best_model_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training step: {str(e)}\")\n",
        "                continue  # Skip this batch and continue with the next one\n",
        "\n",
        "        # Epoch metrics\n",
        "        if running_loss > 0:  # Only if we had successful steps\n",
        "            average_loss = running_loss / len(data_loader)\n",
        "            print(f\"Epoch {epoch+1}, Average Loss: {average_loss:.6f}\")\n",
        "        running_loss = 0.0\n",
        "\n",
        "        if current_step >= num_steps:\n",
        "            break\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    final_path = os.path.join(save_dir, 'GlowTTS_final_model.pth')\n",
        "    torch.save({\n",
        "        'step': current_step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': average_loss\n",
        "    }, final_path)"
      ],
      "metadata": {
        "id": "C6-mOkkkv5ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and data loader\n",
        "model = TextToSSL()\n",
        "save_dir = '/content/drive/MyDrive/SSL_TTS_Checkpoints'\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/MyDrive')\n",
        "\n",
        "# Train from scratch\n",
        "train_model_with_checkpoints(model, data_loader, save_dir)\n",
        "\n",
        "# Or resume from a checkpoint\n",
        "# train_model_with_checkpoints(\n",
        "#     model,\n",
        "#     data_loader,\n",
        "#     save_dir,\n",
        "#     resume_from='/content/drive/MyDrive/SSL_TTS_Checkpoints/checkpoint_name.pth'\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "_O5c_WzpI3ES",
        "outputId": "06c63612-506c-4784-a1f9-69fe6f12699c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: torch.Size([4, 351, 1024])\n",
            "Text tensor shape: torch.Size([4, 111])\n",
            "Text lengths: tensor([ 84, 100, 111, 100], device='cuda:0')\n",
            "Available output keys: dict_keys(['z', 'z_m', 'z_logs', 'logdet', 'z_mask', 'logw', 'logw_', 'attn', 'ssl_features'])\n",
            "z shape: torch.Size([4, 350, 1024])\n",
            "logdet shape: torch.Size([4])\n",
            "attn shape: torch.Size([4, 350, 111])\n",
            "ssl_features shape: torch.Size([4, 350, 1024])\n",
            "Step 250/650000, Loss: 1.3957, Moving Avg Loss: 1.2058, LR: 0.001000, Grad Norm: 0.07\n",
            "Step 500/650000, Loss: 1.0130, Moving Avg Loss: 1.0456, LR: 0.001000, Grad Norm: 0.03\n",
            "Step 750/650000, Loss: 1.1419, Moving Avg Loss: 1.0292, LR: 0.001000, Grad Norm: 0.04\n",
            "Step 1000/650000, Loss: 1.0044, Moving Avg Loss: 1.0425, LR: 0.001000, Grad Norm: 0.02\n",
            "Step 1250/650000, Loss: 1.0063, Moving Avg Loss: 1.0483, LR: 0.001000, Grad Norm: 0.04\n",
            "Step 1500/650000, Loss: 1.0249, Moving Avg Loss: 1.0325, LR: 0.001000, Grad Norm: 0.01\n",
            "Step 1750/650000, Loss: 0.8973, Moving Avg Loss: 1.0491, LR: 0.001000, Grad Norm: 0.04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d36a912e4ca5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_model_with_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Or resume from a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-5ff1552afc91>\u001b[0m in \u001b[0;36mtrain_model_with_checkpoints\u001b[0;34m(model, data_loader, save_dir, num_steps, save_interval, device, resume_from)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9e17aaa88619>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Extract WavLM features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssl_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Get the corresponding text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-4adf0209b0e3>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, waveform, sample_rate)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Move waveform to the specified device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-NN Retrieval System"
      ],
      "metadata": {
        "id": "HZVo4IxMeAGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements the k-Nearest Neighbors retrieval mechanism for voice conversion:\n",
        "\n",
        "### Technical Features\n",
        "1. Efficient Batch Processing:\n",
        "   - Handles multiple sequences simultaneously\n",
        "   - Optimized matrix operations\n",
        "   - Memory-efficient implementation\n",
        "\n",
        "2. Distance Calculation (more below):\n",
        "   - Cosine similarity metric\n",
        "   - Numerical stability handling\n",
        "   - Batch matrix multiplication\n",
        "\n",
        "3. Feature Averaging:\n",
        "   - Uniform weighting of k-nearest neighbors\n",
        "   - Proper dimension handling\n",
        "   - Gradient-free operations\n",
        "\n",
        "### Parameters\n",
        "- k: Number of neighbors (default: 4)\n",
        "- device: Computation device\n",
        "- input dimensions: [B, T, D] for both source and target\n",
        "- output dimensions: [B, T, D] for selected features\n",
        "\n",
        "### Cosine Similarity\n",
        "For two feature vectors $\\mathbf{a}$ and $\\mathbf{b}$ in a high-dimensional space (in our case, $\\mathbb{R}^{1024}$), the cosine similarity is defined as:\n",
        "\n",
        "$\n",
        "\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{a} \\cdot \\mathbf{b}$ is the dot product\n",
        "- $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$ are the L2 norms (Euclidean norms)\n",
        "\n",
        "For batched computation with source features $\\mathbf{S} \\in \\mathbb{R}^{B \\times T_s \\times D}$ and target features $\\mathbf{T} \\in \\mathbb{R}^{B \\times T_t \\times D}$, we compute:\n",
        "\n",
        "$\n",
        "\\text{Similarity}_{batch} = \\frac{\\mathbf{S}\\mathbf{T}^T}{\\|\\mathbf{S}\\|_2 \\|\\mathbf{T}\\|_2^T}\n",
        "$\n",
        "\n",
        "### Cosine Distance\n",
        "The cosine distance is derived from the cosine similarity:\n",
        "\n",
        "$\n",
        "d_{cos}(\\mathbf{a}, \\mathbf{b}) = 1 - \\cos(\\mathbf{a}, \\mathbf{b})\n",
        "$\n",
        "\n",
        "In our implementation, we compute this in steps:\n",
        "\n",
        "1. **Dot Product**:\n",
        "   $\\text{dot}_{batch} = \\mathbf{S}\\mathbf{T}^T \\in \\mathbb{R}^{B \\times T_s \\times T_t}$\n",
        "\n",
        "2. **L2 Norms**:\n",
        "   $\\|\\mathbf{S}\\|_2 \\in \\mathbb{R}^{B \\times T_s \\times 1}$ and $\\|\\mathbf{T}\\|_2 \\in \\mathbb{R}^{B \\times T_t \\times 1}$\n",
        "\n",
        "3. **Norm Product**:\n",
        "   $\\text{norm\\_prod} = \\|\\mathbf{S}\\|_2\\|\\mathbf{T}\\|_2^T \\in \\mathbb{R}^{B \\times T_s \\times T_t}$\n",
        "\n",
        "4. **Final Distance**:\n",
        "   $d_{cos} = 1 - \\frac{\\text{dot}_{batch}}{\\text{norm\\_prod} + \\epsilon}$\n",
        "\n",
        "where $\\epsilon = 1e-8$ for numerical stability.\n",
        "\n",
        "This distance metric has several advantageous properties for our SSL-TTS framework:\n",
        "\n",
        "1. **Bounded Range**: $d_{cos} \\in [0, 2]$ where:\n",
        "   - 0 indicates identical direction\n",
        "   - 1 indicates orthogonal vectors\n",
        "   - 2 indicates opposite directions\n",
        "\n",
        "2. **Scale Invariance**: The distance is invariant to the magnitude of the vectors, making it suitable for comparing SSL features that may have different magnitudes but similar patterns.\n",
        "\n",
        "3. **Batch Efficiency**: The formulation allows efficient computation across batches using matrix operations, crucial for processing multiple time steps simultaneously.\n"
      ],
      "metadata": {
        "id": "gITtWrXBXxyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNRetrieval:\n",
        "    def __init__(self, k: int = 4, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize KNN retrieval with specified k neighbors.\n",
        "        Args:\n",
        "            k (int): Number of nearest neighbors to retrieve (default: 4 as per paper)\n",
        "            device (str): Device to perform computations on\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.device = device\n",
        "\n",
        "    def _compute_cosine_distance(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute cosine distance between source and target features.\n",
        "\n",
        "        Input shapes:\n",
        "        source: [B, T_s, D] where D is feature dimension (e.g., 1024 for WavLM)\n",
        "        target: [B, T_t, D]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Cosine distance matrix [B, T_s, T_t]\n",
        "        \"\"\"\n",
        "        # 1. Compute dot product: (A·B)\n",
        "        dot_product = torch.bmm(source, target.transpose(1, 2))  # [B, T_s, T_t]\n",
        "\n",
        "        # 2. Compute L2 norms: ||A|| and ||B||\n",
        "        source_norm = torch.norm(source, p=2, dim=2, keepdim=True)  # [B, T_s, 1]\n",
        "        target_norm = torch.norm(target, p=2, dim=2, keepdim=True)  # [B, T_t, 1]\n",
        "\n",
        "        # 3. Compute product of norms: ||A||·||B||\n",
        "        norm_product = torch.bmm(source_norm, target_norm.transpose(1, 2))  # [B, T_s, T_t]\n",
        "\n",
        "        # 4. Compute cosine similarity: (A·B)/(||A||·||B||)\n",
        "        cosine_similarity = dot_product / (norm_product + 1e-8)  # [B, T_s, T_t]\n",
        "\n",
        "        # 5. Convert to distance: 1 - similarity\n",
        "        cosine_distance = 1 - cosine_similarity  # [B, T_s, T_t]\n",
        "\n",
        "        return cosine_distance\n",
        "\n",
        "    def retrieve(self, source_features: torch.Tensor, target_database: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Retrieve k nearest neighbor features from target database.\n",
        "        Args:\n",
        "            source_features (torch.Tensor): Source speaker features [B, T_s, D]\n",
        "            target_database (torch.Tensor): Target speaker features database [B, T_t, D]\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: Selected features [B, T_s, D] and distances [B, T_s, K]\n",
        "        \"\"\"\n",
        "        # Move tensors to device if needed\n",
        "        source_features = source_features.to(self.device)\n",
        "        target_database = target_database.to(self.device)\n",
        "\n",
        "        # Compute distances between source and all target frames\n",
        "        distances = self._compute_cosine_distance(source_features, target_database)  # [B, T_s, T_t]\n",
        "\n",
        "        # Find k nearest neighbors\n",
        "        topk_distances, topk_indices = torch.topk(distances, k=self.k, dim=-1, largest=False)  # [B, T_s, K]\n",
        "\n",
        "        # Get dimensions\n",
        "        B, T_s, _ = source_features.shape\n",
        "        D = target_database.shape[-1]\n",
        "\n",
        "        # Expand indices for batch gathering\n",
        "        batch_indices = torch.arange(B, device=self.device).view(B, 1, 1, 1)\n",
        "        batch_indices = batch_indices.expand(B, T_s, self.k, 1)\n",
        "\n",
        "        # Reshape indices for gathering\n",
        "        topk_indices = topk_indices.unsqueeze(-1)  # [B, T_s, K, 1]\n",
        "\n",
        "        # Combine batch and topk indices\n",
        "        gather_indices = torch.cat([batch_indices, topk_indices], dim=-1)  # [B, T_s, K, 2]\n",
        "\n",
        "        # Gather target features\n",
        "        selected_features = target_database[gather_indices[..., 0], gather_indices[..., 1]]  # [B, T_s, K, D]\n",
        "\n",
        "        # Average the k nearest features (uniform weighting as per paper)\n",
        "        selected_features = selected_features.mean(dim=2)  # [B, T_s, D]\n",
        "\n",
        "        return selected_features, topk_distances\n",
        "\n",
        "    def __call__(self, source_features: torch.Tensor, target_database: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Convenience method to directly get selected features.\n",
        "        \"\"\"\n",
        "        selected_features, _ = self.retrieve(source_features, target_database)\n",
        "        return selected_features"
      ],
      "metadata": {
        "id": "rsdikRZGYHT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\lambda$ function"
      ],
      "metadata": {
        "id": "I_-5vfgQX5SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core Implementation\n",
        "1. Interpolation Formula:\n",
        "```python\n",
        "converted_features = lambda_value * selected_features +\n",
        "                    (1 - lambda_value) * source_features\n",
        "```\n",
        "\n",
        "2. Parameter Management:\n",
        "   - Lambda value bounds checking\n",
        "   - Device handling\n",
        "   - Tensor dimension verification\n",
        "\n",
        "### Features\n",
        "1. Input Validation:\n",
        "   - Lambda range enforcement\n",
        "   - Tensor dimension checking\n",
        "   - Device consistency\n",
        "\n",
        "2. Computation Efficiency:\n",
        "   - In-place operations where possible\n",
        "   - Memory-efficient implementation\n",
        "   - Batch processing support\n",
        "\n",
        "3. Interface Options:\n",
        "   - Direct method call\n",
        "   - Callable interface\n",
        "   - Flexible parameter passing"
      ],
      "metadata": {
        "id": "uHy2g6sgdAAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearInterpolation:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize linear interpolation module.\n",
        "        Args:\n",
        "            device (str): Device to perform computations on\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "\n",
        "    def interpolate(self, selected_features: torch.Tensor, source_features: torch.Tensor, lambda_value: float = 1.0) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Linearly interpolate between selected target features and source features.\n",
        "        Formula: y_converted = λ * y_selected + (1 - λ) * y_source\n",
        "\n",
        "        Args:\n",
        "            selected_features (torch.Tensor): Selected target features from kNN [B, T, D]\n",
        "            source_features (torch.Tensor): Original source features [B, T, D]\n",
        "            lambda_value (float): Interpolation parameter (0 to 1)\n",
        "                                0 = pure source, 1 = pure target (default: 1.0)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Interpolated features [B, T, D]\n",
        "        \"\"\"\n",
        "        # Ensure tensors are on correct device\n",
        "        selected_features = selected_features.to(self.device)\n",
        "        source_features = source_features.to(self.device)\n",
        "\n",
        "        # Ensure lambda is in valid range\n",
        "        lambda_value = max(0.0, min(1.0, lambda_value))\n",
        "\n",
        "        # Perform linear interpolation\n",
        "        converted_features = lambda_value * selected_features + (1 - lambda_value) * source_features\n",
        "\n",
        "        return converted_features\n",
        "\n",
        "    def __call__(self, selected_features: torch.Tensor, source_features: torch.Tensor, lambda_value: float = 1.0) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Convenience method to directly perform interpolation.\n",
        "        \"\"\"\n",
        "        return self.interpolate(selected_features, source_features, lambda_value)"
      ],
      "metadata": {
        "id": "mzFUXZ2QYD8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST no vocoder\n"
      ],
      "metadata": {
        "id": "chI5LrSNdfpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_complete_framework(\n",
        "    ssl_encoder,\n",
        "    text_to_ssl,\n",
        "    knn_retrieval,\n",
        "    linear_interp,\n",
        "    sample_rate: int = 16000,\n",
        "    lambda_values: list = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Test the entire SSL-TTS framework including linear interpolation.\n",
        "\n",
        "    Args:\n",
        "        ssl_encoder: SSL encoder model (WavLM)\n",
        "        text_to_ssl: Text-to-SSL model\n",
        "        knn_retrieval: KNN retrieval module\n",
        "        linear_interp: Linear interpolation module\n",
        "        sample_rate: Audio sample rate (default: 16000)\n",
        "        lambda_values: List of lambda values to test (default: [0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "\n",
        "    Returns:\n",
        "        Dict: Dictionary containing test results and metrics\n",
        "    \"\"\"\n",
        "    device = next(text_to_ssl.parameters()).device\n",
        "    print(f\"\\nRunning complete framework test on device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Create synthetic target audio (2 seconds of audio at 16kHz)\n",
        "        print(\"\\nStep 1: Creating synthetic target audio...\")\n",
        "        target_waveform = torch.randn(1, 2 * sample_rate, device=device)\n",
        "\n",
        "        # 2. Extract SSL features from target audio\n",
        "        print(\"Step 2: Extracting SSL features from target audio...\")\n",
        "        with torch.no_grad():\n",
        "            target_features = ssl_encoder.extract_features(target_waveform, sample_rate)\n",
        "        print(f\"Target features shape: {target_features.shape}\")\n",
        "\n",
        "        # 3. Generate source SSL features from text\n",
        "        print(\"\\nStep 3: Generating source SSL features from text...\")\n",
        "        text_tokens = torch.randint(0, 148, (1, 100), device=device)  # Simulate tokenized text\n",
        "        text_lengths = torch.tensor([100], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            source_outputs = text_to_ssl.generate(text_tokens, text_lengths)\n",
        "            source_features = source_outputs[\"ssl_features\"]\n",
        "        print(f\"Source features shape: {source_features.shape}\")\n",
        "\n",
        "        # 4. Perform k-NN retrieval\n",
        "        print(\"\\nStep 4: Performing k-NN retrieval...\")\n",
        "        selected_features, knn_distances = knn_retrieval.retrieve(source_features, target_features)\n",
        "        print(f\"Selected features shape: {selected_features.shape}\")\n",
        "        print(f\"k-NN distances shape: {knn_distances.shape}\")\n",
        "\n",
        "        # 5. Test different lambda values for interpolation\n",
        "        print(\"\\nStep 5: Testing linear interpolation with different λ values...\")\n",
        "        interpolation_results = {}\n",
        "\n",
        "        for lambda_val in lambda_values:\n",
        "            print(f\"\\nTesting λ = {lambda_val}\")\n",
        "            converted_features = linear_interp(\n",
        "                selected_features=selected_features,\n",
        "                source_features=source_features,\n",
        "                lambda_value=lambda_val\n",
        "            )\n",
        "\n",
        "            # Compute metrics for this lambda value\n",
        "            metrics = {\n",
        "                \"converted_shape\": converted_features.shape,\n",
        "                \"converted_mean\": converted_features.mean().item(),\n",
        "                \"converted_std\": converted_features.std().item(),\n",
        "                \"source_similarity\": torch.nn.functional.cosine_similarity(\n",
        "                    converted_features.view(-1, converted_features.size(-1)),\n",
        "                    source_features.view(-1, source_features.size(-1)),\n",
        "                    dim=-1\n",
        "                ).mean().item(),\n",
        "                # For target similarity, we'll compute it frame by frame\n",
        "                \"target_similarity\": torch.nn.functional.cosine_similarity(\n",
        "                    converted_features.mean(dim=1),  # Average over time dimension first\n",
        "                    target_features.mean(dim=1),     # Average over time dimension first\n",
        "                    dim=-1\n",
        "                ).mean().item(),\n",
        "                \"min_knn_distance\": knn_distances.min().item(),\n",
        "                \"max_knn_distance\": knn_distances.max().item(),\n",
        "                \"mean_knn_distance\": knn_distances.mean().item()\n",
        "            }\n",
        "\n",
        "            interpolation_results[lambda_val] = metrics\n",
        "\n",
        "            print(f\"Shape: {metrics['converted_shape']}\")\n",
        "            print(f\"Mean: {metrics['converted_mean']:.3f}, Std: {metrics['converted_std']:.3f}\")\n",
        "            print(f\"Similarity to source: {metrics['source_similarity']:.3f}\")\n",
        "            print(f\"Similarity to target: {metrics['target_similarity']:.3f}\")\n",
        "\n",
        "        # 6. Compile final results\n",
        "        final_results = {\n",
        "            \"feature_shapes\": {\n",
        "                \"source\": source_features.shape,\n",
        "                \"target\": target_features.shape,\n",
        "                \"selected\": selected_features.shape\n",
        "            },\n",
        "            \"knn_metrics\": {\n",
        "                \"min_distance\": knn_distances.min().item(),\n",
        "                \"max_distance\": knn_distances.max().item(),\n",
        "                \"mean_distance\": knn_distances.mean().item()\n",
        "            },\n",
        "            \"interpolation_results\": interpolation_results\n",
        "        }\n",
        "\n",
        "        print(\"\\nTest completed successfully!\")\n",
        "        return final_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest failed with error: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run_complete_framework_test():\n",
        "    \"\"\"\n",
        "    Run the complete framework test with all components\n",
        "    \"\"\"\n",
        "    # Initialize models\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    try:\n",
        "        print(\"Initializing components...\")\n",
        "        ssl_encoder = SSLEncoder(device=device)\n",
        "        text_to_ssl = TextToSSL().to(device)\n",
        "        knn_retrieval = KNNRetrieval(k=4, device=device)\n",
        "        linear_interp = LinearInterpolation(device=device)\n",
        "\n",
        "        # Run complete test\n",
        "        results = test_complete_framework(\n",
        "            ssl_encoder=ssl_encoder,\n",
        "            text_to_ssl=text_to_ssl,\n",
        "            knn_retrieval=knn_retrieval,\n",
        "            linear_interp=linear_interp\n",
        "        )\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nFinal Summary:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(\"\\nFeature Shapes:\")\n",
        "        for name, shape in results[\"feature_shapes\"].items():\n",
        "            print(f\"{name}: {shape}\")\n",
        "\n",
        "        print(\"\\nk-NN Metrics:\")\n",
        "        for name, value in results[\"knn_metrics\"].items():\n",
        "            print(f\"{name}: {value:.3f}\")\n",
        "\n",
        "        print(\"\\nInterpolation Results Summary:\")\n",
        "        for lambda_val, metrics in results[\"interpolation_results\"].items():\n",
        "            print(f\"\\nλ = {lambda_val}:\")\n",
        "            print(f\"Source similarity: {metrics['source_similarity']:.3f}\")\n",
        "            print(f\"Target similarity: {metrics['target_similarity']:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Framework test failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_complete_framework_test()"
      ],
      "metadata": {
        "id": "hQUZ9o10KwZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocoder"
      ],
      "metadata": {
        "id": "UGR_YD6OYPs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implements the HiFi-GAN vocoder for waveform generation:\n",
        "\n",
        "### Technical Details\n",
        "1. Model Components:\n",
        "   - Residual blocks\n",
        "   - Upsampling layers\n",
        "   - Convolutional layers\n",
        "\n",
        "2. Audio Generation:\n",
        "   - Feature conditioning\n",
        "   - Multi-scale processing\n",
        "   - Waveform synthesis\n",
        "\n",
        "3. Current Status:\n",
        "   - Checkpoint loading issue\n",
        "   - Needs path configuration\n",
        "   - Testing infrastructure ready"
      ],
      "metadata": {
        "id": "3xX5nZOWh8uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from TTS.vocoder.models.hifigan_generator import HifiganGenerator\n",
        "\n",
        "class Vocoder:\n",
        "    def __init__(self, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize HiFi-GAN vocoder.\n",
        "        Args:\n",
        "            device (str): Device to perform computations on\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        print(f\"Initializing HiFi-GAN vocoder on {device}...\")\n",
        "\n",
        "        # Initialize HiFi-GAN with configuration from the paper\n",
        "        self.model = HifiganGenerator(\n",
        "            in_channels=1024,  # WavLM feature dimension\n",
        "            out_channels=1,    # Single channel audio output\n",
        "            resblock_type=\"1\", # NOT SURE WE NEED ALL THESE, I THINK WE KEEP DEFAULT SO WE CAN REMOVE ALL THESE:\n",
        "            resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
        "            resblock_kernel_sizes=[3, 7, 11],\n",
        "            upsample_kernel_sizes=[16, 16, 4, 4],\n",
        "            upsample_initial_channel=512,\n",
        "            upsample_factors=[8, 8, 2, 2],\n",
        "        ).to(device)\n",
        "\n",
        "        # Load pre-trained weights\n",
        "        self._load_pretrained_weights()\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        self.model.eval()\n",
        "        print(\"Vocoder initialized successfully!\")\n",
        "\n",
        "    def _load_pretrained_weights(self):\n",
        "        \"\"\"Load pre-trained HiFi-GAN weights\"\"\"\n",
        "        checkpoint_path = \"path/to/hifigan_checkpoint.pth\"  # Replace with actual path\n",
        "        self.model.load_checkpoint(config=None, checkpoint_path=checkpoint_path, eval=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate waveform from SSL features.\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor): SSL features [B, T, D]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Generated waveform [B, 1, T*hop_length]\n",
        "        \"\"\"\n",
        "        # Ensure features are on correct device\n",
        "        features = features.to(self.device)\n",
        "\n",
        "        # Move channel dimension to match HiFi-GAN input requirements\n",
        "        features = features.transpose(1, 2)  # [B, T, D] -> [B, D, T]\n",
        "\n",
        "        # Generate audio\n",
        "        waveform = self.model.inference(features)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "    def __call__(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convenience method to directly generate waveform.\"\"\"\n",
        "        return self.generate(features)\n",
        "\n",
        "def test_vocoder():\n",
        "    \"\"\"Test the vocoder with synthetic features.\"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Initialize vocoder\n",
        "    vocoder = Vocoder(device=device)\n",
        "\n",
        "    # Create synthetic feature input\n",
        "    B, T, D = 1, 100, 1024  # Batch, Time, Features\n",
        "    features = torch.randn(B, T, D, device=device)\n",
        "\n",
        "    try:\n",
        "        # Generate waveform\n",
        "        waveform = vocoder(features)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nVocoder Test Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Input features shape: {features.shape}\")\n",
        "        print(f\"Output waveform shape: {waveform.shape}\")\n",
        "        print(f\"Waveform statistics:\")\n",
        "        print(f\"  Mean: {waveform.mean().item():.3f}\")\n",
        "        print(f\"  Std: {waveform.std().item():.3f}\")\n",
        "        print(f\"  Min: {waveform.min().item():.3f}\")\n",
        "        print(f\"  Max: {waveform.max().item():.3f}\")\n",
        "\n",
        "        # Optional: Save generated audio\n",
        "        # torchaudio.save('test_output.wav', waveform.cpu(), sample_rate=16000)\n",
        "\n",
        "        print(\"\\nTest completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest failed with error: {str(e)}\")\n",
        "        raise\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKjP15biYYd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.vocoder.models.hifigan_generator import HifiganGenerator\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "class Vocoder:\n",
        "    def __init__(self, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize HiFi-GAN vocoder.\n",
        "        Args:\n",
        "            device (str): Device to perform computations on\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        print(f\"Initializing HiFi-GAN vocoder on {device}...\")\n",
        "\n",
        "        # Initialize HiFi-GAN with configuration from the paper\n",
        "        self.model = HifiganGenerator(\n",
        "            in_channels=1024,  # WavLM feature dimension\n",
        "            out_channels=1,    # Single channel audio output\n",
        "            resblock_type=\"1\",\n",
        "            resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
        "            resblock_kernel_sizes=[3, 7, 11],\n",
        "            upsample_kernel_sizes=[16, 16, 4, 4],\n",
        "            upsample_initial_channel=512,\n",
        "            upsample_factors=[8, 8, 2, 2],\n",
        "        ).to(device)\n",
        "\n",
        "        # Load pre-trained weights\n",
        "        self._load_pretrained_weights()\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        self.model.eval()\n",
        "        print(\"Vocoder initialized successfully!\")\n",
        "\n",
        "    def _download_checkpoint(self, url, filename):\n",
        "        \"\"\"Download checkpoint if it doesn't exist\"\"\"\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"Downloading {filename}...\")\n",
        "            with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                                   miniters=1, desc=filename) as t:\n",
        "                urllib.request.urlretrieve(url, filename=filename,\n",
        "                                         reporthook=t.update_to)\n",
        "\n",
        "    def _load_pretrained_weights(self):\n",
        "        \"\"\"Load pre-trained HiFi-GAN weights\"\"\"\n",
        "        # URL for the prematch generator weights from knn-vc release\n",
        "        url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\"\n",
        "        checkpoint_path = \"prematch_g_02500000.pt\"\n",
        "\n",
        "        # Download the checkpoint if it doesn't exist\n",
        "        self._download_checkpoint(url, checkpoint_path)\n",
        "\n",
        "        # Load the state dict\n",
        "        print(\"Loading checkpoint...\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        # If the checkpoint contains a 'model' or 'generator' key, extract the state dict\n",
        "        if 'model' in checkpoint:\n",
        "            state_dict = checkpoint['model']\n",
        "        elif 'generator' in checkpoint:\n",
        "            state_dict = checkpoint['generator']\n",
        "        else:\n",
        "            state_dict = checkpoint  # Assume it's directly the state dict\n",
        "\n",
        "        # Load the weights into the model\n",
        "        self.model.load_state_dict(state_dict)\n",
        "\n",
        "        # Verify loading was successful\n",
        "        param_sum = sum(p.sum() for p in self.model.parameters())\n",
        "        print(f\"Sum of parameters after loading: {param_sum}\")\n",
        "        assert param_sum != 0, \"Loading failed - all parameters are zero\""
      ],
      "metadata": {
        "id": "22vF3TqD-G5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocoder = Vocoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "frDa-FMG-2YA",
        "outputId": "e41bfb0b-7309-4e34-8f86-0110b4d47a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing HiFi-GAN vocoder on cpu...\n",
            "Downloading prematch_g_02500000.pt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "prematch_g_02500000.pt: 66.2MB [00:01, 59.2MB/s]                            \n",
            "<ipython-input-1-80ea2a01b464>:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for HifiganGenerator:\n\tUnexpected key(s) in state_dict: \"lin_pre.weight\", \"lin_pre.bias\". \n\tsize mismatch for conv_pre.parametrizations.weight.original1: copying a param with shape torch.Size([512, 512, 7]) from checkpoint, the shape in current model is torch.Size([512, 1024, 7]).\n\tsize mismatch for ups.0.parametrizations.weight.original1: copying a param with shape torch.Size([512, 256, 20]) from checkpoint, the shape in current model is torch.Size([512, 256, 16]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-853cf97a1eff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-80ea2a01b464>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Load pre-trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Set to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-80ea2a01b464>\u001b[0m in \u001b[0;36m_load_pretrained_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Load the weights into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Verify loading was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HifiganGenerator:\n\tUnexpected key(s) in state_dict: \"lin_pre.weight\", \"lin_pre.bias\". \n\tsize mismatch for conv_pre.parametrizations.weight.original1: copying a param with shape torch.Size([512, 512, 7]) from checkpoint, the shape in current model is torch.Size([512, 1024, 7]).\n\tsize mismatch for ups.0.parametrizations.weight.original1: copying a param with shape torch.Size([512, 256, 20]) from checkpoint, the shape in current model is torch.Size([512, 256, 16])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn_vc = torch.hub.load(\n",
        "    'bshall/knn-vc',\n",
        "    'knn_vc',\n",
        "    pretrained=True,\n",
        "    prematched=True,\n",
        "    trust_repo=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "AQsds0npTcdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e6544d-4267-4bb0-caea-5ecdd14fb6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/bshall_knn-vc_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing weight norm...\n",
            "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n",
            "WavLM-Large loaded with 315,453,120 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocoder = knn_vc.hifigan\n",
        "print(vocoder)"
      ],
      "metadata": {
        "id": "pW5qXOAYRXBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8335577d-1ff3-4b50-938e-2e2d6ef20a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (lin_pre): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (conv_pre): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "  (ups): ModuleList(\n",
            "    (0): ConvTranspose1d(512, 256, kernel_size=(20,), stride=(10,), padding=(5,))\n",
            "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
            "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
            "  )\n",
            "  (resblocks): ModuleList(\n",
            "    (0): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (1): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (2): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (3): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (4): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (5): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (6): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (7): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (8): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "    (9): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      )\n",
            "    )\n",
            "    (10): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            "      )\n",
            "    )\n",
            "    (11): ResBlock1(\n",
            "      (convs1): ModuleList(\n",
            "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
            "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
            "      )\n",
            "      (convs2): ModuleList(\n",
            "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfQLcshgRWw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Evaluation"
      ],
      "metadata": {
        "id": "UZ7GBNjXTAhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the zero-shot model they used the LibriSpeech test-clean dataset for target speaker reference utterances (ground truth). The database has speech from 20 males and 20 females, 8 minutes of speech per each. We downloaded the data and we specifically need the following file: test-clean, which contains subfolders (one for each speaker) then subfolders within those (one for each chapter of a book that the speakers read from), then the individual audio files (in .flac form, each file is a sentence from the chapter).\n",
        "\n",
        "\\\\\n",
        "\n",
        "To create the output for the model, they passed in 100 English sentences for each speaker, from the FLoRes+ dataset. We downloaded the data and figured out where to find the sentences. We really only need one file, “devtest.eng_Latn”, which contains a multitude of random English sentences. Below you will find example sentences.\n",
        "\n",
        "\\\\\n",
        "\n",
        "MOS = mean opinion score is a measure of the human-judged overall quality of an event or experience. For us, a MOS is a ranking of the quality of speech utterances. Most often judged on a scale of 1 (bad) to 5 (excellent), MOS’s are the average of a number of other human-scored individual parameters. Although originally MOS’s were derived from surveys of expert observers, today a MOS is often produced by an Objective Measurement Method, approximating a human ranking. 4.3-4.5 is considered an excellent target to shoot for due to human tendency to rarely give out perfect 5’s. Below 3.5 is generally unacceptable.\n",
        "\n",
        "\\\\\n",
        "\n",
        "All tests are conducted with $λ=1$. The evaluation focuses on a few key metrics of language:\n",
        "\n",
        "- **Naturalness: UTMOS**\n",
        " - UTMOS = UTokyo-SaruLab Mean Opinion Score, an autonomous method of calculating MOS.\n",
        "\n",
        "- **Intelligibility: WER, PER**\n",
        " - WER = Word Error Rate, i.e. the ratio of word errors in a transcript to the total words spoken. A lower WER in speech-to-text means better accuracy in recognizing speech. In our case, this would be calculated with the formula $\\frac{S+D+I}{N}$, where S is the number of substitutions (instances where a word in the synthesized sentence vector would need to be subsituted to match the truth vector), D is the number of deletions (instances where a word in the synthesized sentence vector would need to be deleted to match the truth vector), I is the number of insertions (instances where a new word would need to be inserted to match the truth vector), and N is the total number of phenomes. The numerator is also known as the edit distance because it represents \"how far away\" two sentences are.\n",
        " - PER = Phenome Error Rate, i.e. the ratio of phenome errors in a transcript to the total phenomes spoken. As above, a lower PER means better accuracy. The formula the same as above, except in the context of comparing phenomes instead of words.\n",
        " - Both of these are calculated using the Whisper-Large v3 model.\n",
        "\n",
        "- **Speaker Similarity: SECS**\n",
        " - SECS = Speaker-Encoder Cosine Similarity, i.e. the cosine similarity between the embeddings of two audio samples, which in our case are a ground truth sample from one speaker and the synthesized sample for that same speaker. The original paper uses ECAPA2 to find these embeddings and their similtarity. The goal of speaker similarity is to determine if two audio samples come from the same spaker, so if the output of the model is above a certain threshold, they are considered to be from the same speaker, otherwise, they are from different speakers\n",
        "\n",
        "- **Subjective Evaluation: N-MOS, S-MOS**\n",
        " - N-MOS = Natural MOS, i.e. how natural the utterance (output) sounds compared to the ground-truth recording.\n",
        " - S-MOS = Similarity MOS, i.e. how similar the utterance sounds compared to the ground-truth recording.\n",
        "The original paper had 10 raters go through 3 synthesized sentences per speaker, thus they went through 60 in total. They then gave a score for each synthesis from 1 to 5 in 0.5 increments. They hired native English speakers in the United States through Amazon Mechanical Turk to rate, so in our case it would just be us 4 rating."
      ],
      "metadata": {
        "id": "QxV83ekgTF00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dev_test = '/content/drive/MyDrive/devtest.eng_Latn'\n",
        "\n",
        "i = 1\n",
        "for sentence in open(dev_test, 'r'):\n",
        "  print(f'Example {i}: {sentence.strip()} \\n')\n",
        "  i+=1\n",
        "  if i==5:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9CYJ8CNUohf",
        "outputId": "5b8803c6-fac9-4dce-85a9-eb42d0924cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Example 1: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added. \n",
            "\n",
            "Example 2: Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days. \n",
            "\n",
            "Example 3: Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes. \n",
            "\n",
            "Example 4: On Monday, Sara Danius, permanent secretary of the Nobel Committee for Literature at the Swedish Academy, publicly announced during a radio program on Sveriges Radio in Sweden the committee, unable to reach Bob Dylan directly about winning the 2016 Nobel Prize in Literature, had abandoned its efforts to reach him. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_clean = '/content/drive/MyDrive/LibriSpeech/test-clean'"
      ],
      "metadata": {
        "id": "U7kaZbxQlSRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}